/home/youhyun/Volume/research/reid_lab/person_reid_lab/HumanBench/PATH/datasets/market1501/data_list/query_list.txt
Successfully Loaded Data, Totally 750 IDs
/home/youhyun/Volume/research/reid_lab/person_reid_lab/HumanBench/PATH/datasets/market1501/data_list/gallery_list.txt
Successfully Loaded Data, Totally 751 IDs
pos embed shape:  torch.Size([1, 128, 768])
sync_print: rank 0, Number of conv/bn params: 0.59M
sync_print: rank 0, Number of linear params: 85.02M
768
[rank 0] add param cls_token as backbone_specific
[rank 0] add param cls_token_pos_embed as backbone_specific
[rank 0] add param pos_embed as backbone_specific
[rank 0] add param patch_embed.proj.weight as backbone_specific
[rank 0] add param patch_embed.proj.bias as backbone_specific
[rank 0] add param blocks.0.norm1.weight as backbone_specific
[rank 0] add param blocks.0.norm1.bias as backbone_specific
[rank 0] add param blocks.0.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.0.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.0.attn.proj.weight as backbone_specific
[rank 0] add param blocks.0.attn.proj.bias as backbone_specific
[rank 0] add param blocks.0.norm2.weight as backbone_specific
[rank 0] add param blocks.0.norm2.bias as backbone_specific
[rank 0] add param blocks.0.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.0.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.0.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.0.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.1.norm1.weight as backbone_specific
[rank 0] add param blocks.1.norm1.bias as backbone_specific
[rank 0] add param blocks.1.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.1.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.1.attn.proj.weight as backbone_specific
[rank 0] add param blocks.1.attn.proj.bias as backbone_specific
[rank 0] add param blocks.1.norm2.weight as backbone_specific
[rank 0] add param blocks.1.norm2.bias as backbone_specific
[rank 0] add param blocks.1.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.1.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.1.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.1.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.2.norm1.weight as backbone_specific
[rank 0] add param blocks.2.norm1.bias as backbone_specific
[rank 0] add param blocks.2.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.2.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.2.attn.proj.weight as backbone_specific
[rank 0] add param blocks.2.attn.proj.bias as backbone_specific
[rank 0] add param blocks.2.norm2.weight as backbone_specific
[rank 0] add param blocks.2.norm2.bias as backbone_specific
[rank 0] add param blocks.2.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.2.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.2.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.2.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.3.norm1.weight as backbone_specific
[rank 0] add param blocks.3.norm1.bias as backbone_specific
[rank 0] add param blocks.3.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.3.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.3.attn.proj.weight as backbone_specific
[rank 0] add param blocks.3.attn.proj.bias as backbone_specific
[rank 0] add param blocks.3.norm2.weight as backbone_specific
[rank 0] add param blocks.3.norm2.bias as backbone_specific
[rank 0] add param blocks.3.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.3.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.3.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.3.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.4.norm1.weight as backbone_specific
[rank 0] add param blocks.4.norm1.bias as backbone_specific
[rank 0] add param blocks.4.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.4.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.4.attn.proj.weight as backbone_specific
[rank 0] add param blocks.4.attn.proj.bias as backbone_specific
[rank 0] add param blocks.4.norm2.weight as backbone_specific
[rank 0] add param blocks.4.norm2.bias as backbone_specific
[rank 0] add param blocks.4.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.4.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.4.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.4.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.5.norm1.weight as backbone_specific
[rank 0] add param blocks.5.norm1.bias as backbone_specific
[rank 0] add param blocks.5.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.5.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.5.attn.proj.weight as backbone_specific
[rank 0] add param blocks.5.attn.proj.bias as backbone_specific
[rank 0] add param blocks.5.norm2.weight as backbone_specific
[rank 0] add param blocks.5.norm2.bias as backbone_specific
[rank 0] add param blocks.5.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.5.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.5.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.5.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.6.norm1.weight as backbone_specific
[rank 0] add param blocks.6.norm1.bias as backbone_specific
[rank 0] add param blocks.6.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.6.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.6.attn.proj.weight as backbone_specific
[rank 0] add param blocks.6.attn.proj.bias as backbone_specific
[rank 0] add param blocks.6.norm2.weight as backbone_specific
[rank 0] add param blocks.6.norm2.bias as backbone_specific
[rank 0] add param blocks.6.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.6.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.6.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.6.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.7.norm1.weight as backbone_specific
[rank 0] add param blocks.7.norm1.bias as backbone_specific
[rank 0] add param blocks.7.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.7.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.7.attn.proj.weight as backbone_specific
[rank 0] add param blocks.7.attn.proj.bias as backbone_specific
[rank 0] add param blocks.7.norm2.weight as backbone_specific
[rank 0] add param blocks.7.norm2.bias as backbone_specific
[rank 0] add param blocks.7.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.7.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.7.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.7.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.8.norm1.weight as backbone_specific
[rank 0] add param blocks.8.norm1.bias as backbone_specific
[rank 0] add param blocks.8.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.8.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.8.attn.proj.weight as backbone_specific
[rank 0] add param blocks.8.attn.proj.bias as backbone_specific
[rank 0] add param blocks.8.norm2.weight as backbone_specific
[rank 0] add param blocks.8.norm2.bias as backbone_specific
[rank 0] add param blocks.8.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.8.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.8.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.8.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.9.norm1.weight as backbone_specific
[rank 0] add param blocks.9.norm1.bias as backbone_specific
[rank 0] add param blocks.9.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.9.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.9.attn.proj.weight as backbone_specific
[rank 0] add param blocks.9.attn.proj.bias as backbone_specific
[rank 0] add param blocks.9.norm2.weight as backbone_specific
[rank 0] add param blocks.9.norm2.bias as backbone_specific
[rank 0] add param blocks.9.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.9.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.9.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.9.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.10.norm1.weight as backbone_specific
[rank 0] add param blocks.10.norm1.bias as backbone_specific
[rank 0] add param blocks.10.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.10.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.10.attn.proj.weight as backbone_specific
[rank 0] add param blocks.10.attn.proj.bias as backbone_specific
[rank 0] add param blocks.10.norm2.weight as backbone_specific
[rank 0] add param blocks.10.norm2.bias as backbone_specific
[rank 0] add param blocks.10.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.10.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.10.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.10.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.11.norm1.weight as backbone_specific
[rank 0] add param blocks.11.norm1.bias as backbone_specific
[rank 0] add param blocks.11.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.11.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.11.attn.proj.weight as backbone_specific
[rank 0] add param blocks.11.attn.proj.bias as backbone_specific
[rank 0] add param blocks.11.norm2.weight as backbone_specific
[rank 0] add param blocks.11.norm2.bias as backbone_specific
[rank 0] add param blocks.11.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.11.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.11.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.11.mlp.fc2.bias as backbone_specific
[rank 0] add param norm.weight as backbone_specific
[rank 0] add param norm.bias as backbone_specific
[rank 0] add param reduction_layers.0.weight as neck_specific
[rank 0] add param reduction_layers.0.bias as neck_specific
[rank 0] add param reduction_layers.1.weight as neck_specific
[rank 0] add param reduction_layers.1.bias as neck_specific
[rank 0] add param reduction_layers.2.weight as neck_specific
[rank 0] add param reduction_layers.2.bias as neck_specific
[rank 0] add param reduction_layers.3.weight as neck_specific
[rank 0] add param reduction_layers.3.bias as neck_specific
[rank 0] add param reduction_layers.4.weight as neck_specific
[rank 0] add param reduction_layers.4.bias as neck_specific
[rank 0] add param reduction_layers.5.weight as neck_specific
[rank 0] add param reduction_layers.5.bias as neck_specific
[rank 0] add param reduction_layers.6.weight as neck_specific
[rank 0] add param reduction_layers.6.bias as neck_specific
[rank 0] add param reduction_layers.7.weight as neck_specific
[rank 0] add param reduction_layers.7.bias as neck_specific
[rank 0] add param reduction_layers.8.weight as neck_specific
[rank 0] add param reduction_layers.8.bias as neck_specific
[rank 0] add param reduction_layers.9.weight as neck_specific
[rank 0] add param reduction_layers.9.bias as neck_specific
[rank 0] add param reduction_layers.10.weight as neck_specific
[rank 0] add param reduction_layers.10.bias as neck_specific
[rank 0] add param reduction_layers.11.weight as neck_specific
[rank 0] add param reduction_layers.11.bias as neck_specific
[rank 0] add param reduction_layers.12.weight as neck_specific
[rank 0] add param reduction_layers.12.bias as neck_specific
[rank 0] add param side_gate_params.0 as neck_specific
[rank 0] add param side_gate_params.1 as neck_specific
[rank 0] add param side_gate_params.2 as neck_specific
[rank 0] add param side_gate_params.3 as neck_specific
[rank 0] add param side_gate_params.4 as neck_specific
[rank 0] add param side_gate_params.5 as neck_specific
[rank 0] add param side_gate_params.6 as neck_specific
[rank 0] add param side_gate_params.7 as neck_specific
[rank 0] add param side_gate_params.8 as neck_specific
[rank 0] add param side_gate_params.9 as neck_specific
[rank 0] add param side_gate_params.10 as neck_specific
[rank 0] add param side_gate_params.11 as neck_specific
[rank 0] add param transformer_blocks.0.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc2.bias as neck_specific
[rank 0] add param last_proj.weight as neck_specific
[rank 0] add param last_proj.bias as neck_specific
[rank 0] add param feat_bn.weight as decoder_specific
[rank 0] add param feat_bn.bias as decoder_specific
[rank 0] add param loss.SoftmaxLoss.classifier.weight as decoder_specific
[rank 0] add buffer feat_bn.running_mean as decoder_specific
[rank 0] add buffer feat_bn.running_var as decoder_specific
[rank 0] add buffer feat_bn.num_batches_tracked as decoder_specific
model_entry(
  (backbone_module): ViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.00909090880304575)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.0181818176060915)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.027272727340459824)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.036363635212183)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.045454543083906174)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.054545458406209946)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.06363636255264282)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.0727272778749466)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.08181818574666977)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.09090909361839294)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): DropPath(p=0.10000000149011612)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_pre): Identity()
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (neck_module): LadderSideAttentionFPN(
    (reduction_layers): ModuleList(
      (0-12): 13 x Linear(in_features=768, out_features=48, bias=True)
    )
    (side_gate_params): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (8): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (9): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (10): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (11): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (transformer_blocks): ModuleList(
      (0-11): 12 x ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=48, out_features=144, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=48, out_features=48, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=48, out_features=192, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=192, out_features=48, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (attention_layers): ModuleList(
      (0-12): 13 x Identity()
    )
    (last_proj): Linear(in_features=48, out_features=768, bias=True)
  )
  (decoder_module): reid_cls_vit_B(
    (feat_bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (loss): Softmax(in_features=768, out_features=1501)TripletLoss(, margin=0.3)Softmax_TripletLoss(, balance_weight=1.0)
  )
)
[rank 0] broadcasting backbone-specific param module.backbone_module.cls_token	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.cls_token_pos_embed	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.pos_embed	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.patch_embed.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.patch_embed.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.qkv.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.norm.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting backbone-specific param module.backbone_module.norm.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c5e65430>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.0.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.0.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.3.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.3.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.4.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.4.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.5.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.5.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.6.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.6.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.7.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.7.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.8.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.8.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.9.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.9.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.10.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.10.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.11.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.11.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.12.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.12.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.0	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.1	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.2	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.3	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.4	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.5	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.6	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.7	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.8	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.9	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.10	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.11	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.qkv.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc1.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc1.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc2.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc2.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.last_proj.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting neck-specific param module.neck_module.last_proj.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524ebb0>
[rank 0] broadcasting decoder-specific param module.decoder_module.feat_bn.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524e830>
[rank 0] broadcasting decoder-specific param module.decoder_module.feat_bn.bias	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524e830>
[rank 0] broadcasting decoder-specific param module.decoder_module.loss.SoftmaxLoss.classifier.weight	group_idx=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ca2c524e830>
[rank 0] broadcasting decoder-specific buffer module.decoder_module.feat_bn.running_mean
[rank 0] broadcasting decoder-specific buffer module.decoder_module.feat_bn.running_var
[rank 0] broadcasting decoder-specific buffer module.decoder_module.feat_bn.num_batches_tracked
=> loading checkpoint '/home/youhyun/Volume/research/reid_lab/person_reid_lab/HumanBench/PATH/asset/weights/PATH-ViT-B.pth'
caution: module.backbone_module.blocks.3.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.attn.proj.bias not loaded
caution: module.backbone_module.blocks.6.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.8.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.10.0.attn.qkv.weight not loaded
caution: module.backbone_module.blocks.1.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.9.norm2.weight not loaded
caution: module.neck_module.side_gate_params.7 not loaded
caution: module.neck_module.transformer_blocks.0.0.attn.qkv.weight not loaded
caution: module.backbone_module.blocks.1.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.3.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.5.0.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.2.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.7.norm1.weight not loaded
caution: module.neck_module.last_proj.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.norm2.weight not loaded
caution: module.backbone_module.blocks.9.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.5.norm2.weight not loaded
caution: module.neck_module.reduction_layers.4.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.norm1.weight not loaded
caution: module.backbone_module.blocks.6.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.6.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.7.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.10.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.10.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.2.norm2.weight not loaded
caution: module.neck_module.reduction_layers.6.weight not loaded
caution: module.backbone_module.blocks.11.norm2.weight not loaded
caution: module.neck_module.reduction_layers.9.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.9.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.1.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.5.0.norm2.weight not loaded
caution: module.neck_module.side_gate_params.6 not loaded
caution: module.backbone_module.blocks.3.attn.qkv.weight not loaded
caution: module.backbone_module.blocks.2.norm1.weight not loaded
caution: module.backbone_module.blocks.11.mlp.fc1.weight not loaded
caution: module.neck_module.reduction_layers.3.weight not loaded
caution: module.neck_module.transformer_blocks.8.0.norm2.weight not loaded
caution: module.backbone_module.blocks.11.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.0.0.mlp.fc1.weight not loaded
caution: module.neck_module.reduction_layers.10.weight not loaded
caution: module.neck_module.reduction_layers.8.weight not loaded
caution: module.neck_module.transformer_blocks.7.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.attn.proj.bias not loaded
caution: module.backbone_module.blocks.0.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.8.attn.qkv.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.4.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.norm1.weight not loaded
caution: module.neck_module.reduction_layers.2.bias not loaded
caution: module.decoder_module.loss.SoftmaxLoss.classifier.weight not loaded
caution: module.neck_module.reduction_layers.7.bias not loaded
caution: module.neck_module.transformer_blocks.1.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.9.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.6.attn.proj.bias not loaded
caution: module.backbone_module.blocks.1.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.3.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.5.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.2.0.attn.proj.bias not loaded
caution: module.backbone_module.blocks.8.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.11.0.norm1.weight not loaded
caution: module.backbone_module.blocks.10.attn.qkv.weight not loaded
caution: module.backbone_module.norm.bias not loaded
caution: module.backbone_module.blocks.2.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.7.0.norm1.weight not loaded
caution: module.neck_module.reduction_layers.8.bias not loaded
caution: module.backbone_module.blocks.6.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.norm2.bias not loaded
caution: module.backbone_module.blocks.3.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.attn.proj.bias not loaded
caution: module.backbone_module.blocks.3.norm2.bias not loaded
caution: module.backbone_module.blocks.1.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.0.0.mlp.fc2.weight not loaded
caution: module.neck_module.reduction_layers.11.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.norm1.weight not loaded
caution: module.backbone_module.blocks.9.norm2.bias not loaded
caution: module.backbone_module.blocks.0.attn.qkv.weight not loaded
caution: module.decoder_module.feat_bn.bias not loaded
caution: module.backbone_module.blocks.11.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.6.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.10.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.norm2.bias not loaded
caution: module.backbone_module.blocks.7.attn.qkv.bias not loaded
caution: module.neck_module.reduction_layers.12.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.mlp.fc1.weight not loaded
caution: module.neck_module.side_gate_params.9 not loaded
caution: module.neck_module.transformer_blocks.3.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.6.0.norm2.weight not loaded
caution: module.neck_module.reduction_layers.7.weight not loaded
caution: module.backbone_module.blocks.0.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.6.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.norm2.bias not loaded
caution: module.backbone_module.blocks.4.norm1.weight not loaded
caution: module.backbone_module.blocks.3.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.4.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.11.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.10.0.attn.proj.weight not loaded
caution: module.neck_module.reduction_layers.4.weight not loaded
caution: module.backbone_module.cls_token_pos_embed not loaded
caution: module.neck_module.transformer_blocks.10.0.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.3.norm1.bias not loaded
caution: module.backbone_module.blocks.11.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.norm1.bias not loaded
caution: module.backbone_module.blocks.6.attn.proj.weight not loaded
caution: module.backbone_module.blocks.5.norm2.bias not loaded
caution: module.backbone_module.blocks.11.attn.proj.bias not loaded
caution: module.backbone_module.blocks.9.mlp.fc2.weight not loaded
caution: module.neck_module.side_gate_params.2 not loaded
caution: module.neck_module.transformer_blocks.5.0.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.11.mlp.fc1.bias not loaded
caution: module.neck_module.reduction_layers.3.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.mlp.fc2.bias not loaded
caution: module.neck_module.last_proj.weight not loaded
caution: module.backbone_module.blocks.5.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.5.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.3.0.norm1.weight not loaded
caution: module.backbone_module.blocks.4.norm2.weight not loaded
caution: module.backbone_module.blocks.7.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.7.norm2.weight not loaded
caution: module.backbone_module.blocks.2.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.2.attn.qkv.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.10.attn.proj.bias not loaded
caution: module.backbone_module.blocks.7.attn.proj.weight not loaded
caution: module.backbone_module.blocks.7.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.11.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.1.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.11.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.5.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.10.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.norm2.weight not loaded
caution: module.backbone_module.blocks.10.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.8.0.norm1.bias not loaded
caution: module.backbone_module.blocks.10.attn.proj.weight not loaded
caution: module.backbone_module.blocks.8.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.2.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.norm2.weight not loaded
caution: module.backbone_module.pos_embed not loaded
caution: module.neck_module.transformer_blocks.4.0.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.norm1.weight not loaded
caution: module.backbone_module.blocks.4.attn.proj.bias not loaded
caution: module.backbone_module.blocks.9.attn.qkv.weight not loaded
caution: module.neck_module.reduction_layers.9.weight not loaded
caution: module.backbone_module.blocks.8.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.0.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.9.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.5.0.norm1.bias not loaded
caution: module.backbone_module.blocks.6.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.5.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.9.attn.proj.weight not loaded
caution: module.backbone_module.blocks.0.norm1.bias not loaded
caution: module.backbone_module.blocks.11.norm1.weight not loaded
caution: module.neck_module.side_gate_params.11 not loaded
caution: module.neck_module.transformer_blocks.1.0.norm2.weight not loaded
caution: module.backbone_module.blocks.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.9.0.attn.qkv.weight not loaded
caution: module.neck_module.reduction_layers.12.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.1.norm1.bias not loaded
caution: module.backbone_module.blocks.6.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.1.norm2.weight not loaded
caution: module.neck_module.reduction_layers.5.weight not loaded
caution: module.backbone_module.blocks.4.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.10.0.norm2.bias not loaded
caution: module.backbone_module.blocks.9.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.1.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.4.0.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.5.0.norm2.bias not loaded
caution: module.backbone_module.blocks.8.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.3.mlp.fc2.weight not loaded
caution: module.neck_module.side_gate_params.0 not loaded
caution: module.backbone_module.blocks.7.mlp.fc2.weight not loaded
caution: module.backbone_module.patch_embed.proj.weight not loaded
caution: module.backbone_module.blocks.4.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.3.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.7.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.11.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.0.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.9.0.norm2.bias not loaded
caution: module.neck_module.side_gate_params.3 not loaded
caution: module.neck_module.transformer_blocks.5.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.mlp.fc2.bias not loaded
caution: module.neck_module.reduction_layers.10.bias not loaded
caution: module.neck_module.transformer_blocks.5.0.attn.proj.bias not loaded
caution: module.neck_module.reduction_layers.5.bias not loaded
caution: module.backbone_module.blocks.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.7.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.2.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.8.0.attn.qkv.weight not loaded
caution: module.backbone_module.blocks.11.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.8.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.4.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.0.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.8.0.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.10.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.norm2.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.norm1.bias not loaded
caution: module.backbone_module.blocks.5.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.2.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.3.0.mlp.fc1.weight not loaded
caution: module.backbone_module.norm.weight not loaded
caution: module.backbone_module.blocks.5.attn.proj.bias not loaded
caution: module.backbone_module.blocks.9.norm1.weight not loaded
caution: module.backbone_module.blocks.10.norm1.bias not loaded
caution: module.backbone_module.blocks.4.attn.qkv.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.attn.qkv.weight not loaded
caution: module.neck_module.reduction_layers.1.weight not loaded
caution: module.neck_module.transformer_blocks.0.0.attn.proj.weight not loaded
caution: module.backbone_module.blocks.7.norm1.bias not loaded
caution: module.neck_module.side_gate_params.5 not loaded
caution: module.backbone_module.blocks.6.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.0.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.2.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.3.norm1.weight not loaded
caution: module.backbone_module.blocks.9.mlp.fc2.bias not loaded
caution: module.neck_module.side_gate_params.1 not loaded
caution: module.neck_module.transformer_blocks.9.0.norm2.weight not loaded
caution: module.backbone_module.blocks.6.norm2.bias not loaded
caution: module.backbone_module.blocks.1.attn.qkv.weight not loaded
caution: module.decoder_module.feat_bn.weight not loaded
caution: module.backbone_module.blocks.5.attn.proj.weight not loaded
caution: module.neck_module.transformer_blocks.8.0.attn.proj.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.norm1.bias not loaded
caution: module.backbone_module.blocks.9.norm1.bias not loaded
caution: module.neck_module.side_gate_params.8 not loaded
caution: module.neck_module.reduction_layers.0.bias not loaded
caution: module.backbone_module.blocks.1.norm2.bias not loaded
caution: module.backbone_module.blocks.9.mlp.fc1.bias not loaded
caution: module.backbone_module.blocks.11.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.3.norm2.weight not loaded
caution: module.backbone_module.blocks.4.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.0.attn.proj.bias not loaded
caution: module.backbone_module.blocks.10.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.11.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.mlp.fc2.weight not loaded
caution: module.neck_module.transformer_blocks.1.0.norm2.bias not loaded
caution: module.backbone_module.blocks.2.norm2.bias not loaded
caution: module.backbone_module.blocks.6.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.8.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.3.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.mlp.fc2.weight not loaded
caution: module.backbone_module.blocks.5.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.8.attn.proj.weight not loaded
caution: module.backbone_module.blocks.7.attn.proj.bias not loaded
caution: module.backbone_module.blocks.6.attn.qkv.bias not loaded
caution: module.neck_module.transformer_blocks.2.0.attn.proj.weight not loaded
caution: module.neck_module.side_gate_params.4 not loaded
caution: module.neck_module.transformer_blocks.6.0.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.8.norm2.bias not loaded
caution: module.backbone_module.blocks.1.attn.proj.bias not loaded
caution: module.backbone_module.blocks.4.norm1.bias not loaded
caution: module.backbone_module.blocks.8.mlp.fc1.bias not loaded
caution: module.backbone_module.cls_token not loaded
caution: module.neck_module.transformer_blocks.2.0.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.1.0.norm1.weight not loaded
caution: module.neck_module.transformer_blocks.5.0.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.9.attn.qkv.bias not loaded
caution: module.backbone_module.blocks.4.mlp.fc1.weight not loaded
caution: module.backbone_module.blocks.8.norm1.weight not loaded
caution: module.neck_module.reduction_layers.2.weight not loaded
caution: module.neck_module.reduction_layers.1.bias not loaded
caution: module.backbone_module.blocks.2.norm1.bias not loaded
caution: module.neck_module.transformer_blocks.6.0.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.11.0.mlp.fc1.bias not loaded
caution: module.neck_module.reduction_layers.11.bias not loaded
caution: module.backbone_module.blocks.4.mlp.fc1.bias not loaded
caution: module.neck_module.transformer_blocks.5.0.attn.qkv.weight not loaded
caution: module.neck_module.transformer_blocks.4.0.norm1.weight not loaded
caution: module.backbone_module.blocks.8.mlp.fc2.bias not loaded
caution: module.neck_module.transformer_blocks.7.0.norm2.weight not loaded
caution: module.backbone_module.blocks.10.norm2.weight not loaded
caution: module.neck_module.side_gate_params.10 not loaded
caution: module.backbone_module.blocks.3.attn.proj.weight not loaded
caution: module.backbone_module.blocks.10.mlp.fc2.weight not loaded
caution: module.neck_module.reduction_layers.6.bias not loaded
caution: module.backbone_module.blocks.0.attn.proj.weight not loaded
caution: module.backbone_module.patch_embed.proj.bias not loaded
caution: module.backbone_module.blocks.2.attn.proj.bias not loaded
caution: module.backbone_module.blocks.4.norm2.bias not loaded
caution: module.backbone_module.blocks.5.mlp.fc2.bias not loaded
caution: module.backbone_module.blocks.0.mlp.fc1.bias not loaded
caution: module.neck_module.reduction_layers.0.weight not loaded
caution: module.backbone_module.blocks.4.attn.proj.weight not loaded
caution: module.backbone_module.blocks.1.norm1.weight not loaded
caution: module.backbone_module.blocks.10.norm2.bias not loaded
caution: module.backbone_module.blocks.7.norm2.bias not loaded
caution: module.neck_module.transformer_blocks.5.0.norm1.weight not loaded
caution: module.backbone_module.blocks.5.norm1.weight not loaded
caution: module.backbone_module.blocks.7.mlp.fc1.weight not loaded
caution: module.neck_module.transformer_blocks.2.0.norm1.bias not loaded
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [1/27]	task0 : reid	Time 16.873 (ETA:0.58h) (15.366)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [2/27]	task0 : reid	Time 9.127 (ETA:0.31h) (7.684)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [3/27]	task0 : reid	Time 6.526 (ETA:0.22h) (5.124)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [4/27]	task0 : reid	Time 5.216 (ETA:0.18h) (3.844)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [5/27]	task0 : reid	Time 4.451 (ETA:0.15h) (3.076)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [6/27]	task0 : reid	Time 3.941 (ETA:0.13h) (2.563)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [7/27]	task0 : reid	Time 3.564 (ETA:0.12h) (2.198)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [8/27]	task0 : reid	Time 3.282 (ETA:0.11h) (1.923)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [9/27]	task0 : reid	Time 3.062 (ETA:0.10h) (1.710)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [10/27]	task0 : reid	Time 2.887 (ETA:0.09h) (1.539)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [11/27]	task0 : reid	Time 1.330 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [12/27]	task0 : reid	Time 1.322 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [13/27]	task0 : reid	Time 1.320 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [14/27]	task0 : reid	Time 1.321 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [15/27]	task0 : reid	Time 1.311 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [16/27]	task0 : reid	Time 1.302 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [17/27]	task0 : reid	Time 1.302 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [18/27]	task0 : reid	Time 1.301 (ETA:0.04h) (0.004)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [19/27]	task0 : reid	Time 1.300 (ETA:0.04h) (0.004)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [20/27]	task0 : reid	Time 1.300 (ETA:0.04h) (0.004)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [21/27]	task0 : reid	Time 1.299 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [22/27]	task0 : reid	Time 1.299 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [23/27]	task0 : reid	Time 1.305 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [24/27]	task0 : reid	Time 1.314 (ETA:0.04h) (0.003)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [25/27]	task0 : reid	Time 1.323 (ETA:0.04h) (0.004)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [26/27]	task0 : reid	Time 1.332 (ETA:0.04h) (0.004)	
dict_keys(['image', 'label', 'camera', 'index', 'backbone_output', 'model_args', 'neck_output'])
Extract Features: [27/27]	task0 : reid	Time 1.220 (ETA:0.03h) (0.004)	
